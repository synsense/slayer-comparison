{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sinabs.activation as sa\n",
    "from sinabs.slayer.layers import LIF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from slayer_layer import SlayerLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 250\n",
    "hidden_dim = 25\n",
    "tau_mem = 20.\n",
    "spike_threshold = 0.01\n",
    "learning_rate = 1e-5\n",
    "n_time_steps = 200\n",
    "epochs = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlayerNet(torch.nn.Module):\n",
    "    def __init__(self, n_time_steps, encoding_dim, hidden_dim, tau_mem, spike_threshold, width_grad, scale_grad):\n",
    "        super().__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "            \"type\": \"CUBALIF\",\n",
    "            \"theta\": spike_threshold,\n",
    "            \"tauSr\": tau_mem,\n",
    "            \"tauRef\": tau_mem,\n",
    "            \"scaleRef\": 1,\n",
    "            \"tauRho\": width_grad,\n",
    "            \"scaleRho\": scale_grad,\n",
    "        }\n",
    "        sim_params = {\"Ts\": 1.0, \"tSample\": n_time_steps}\n",
    "        self.slayer = SlayerLayer(neuron_params, sim_params)\n",
    "        self.lin1 = self.slayer.dense(encoding_dim, hidden_dim)\n",
    "        self.lin2 = self.slayer.dense(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weighted = self.lin1(x)\n",
    "        psp = self.slayer.psp(self.weighted)\n",
    "        self.psp_pre = psp.clone()\n",
    "        out = self.slayer.spike(psp)\n",
    "        self.psp_post = psp.clone()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "act_fn = sa.ActivationFunction(\n",
    "            spike_threshold=spike_threshold,\n",
    "            spike_fn=sa.SingleSpike,\n",
    "            reset_fn=sa.MembraneSubtract(),\n",
    "            surrogate_grad_fn=sa.SingleExponential(),\n",
    "        )\n",
    "\n",
    "exodus_model = nn.Sequential(\n",
    "                nn.Linear(encoding_dim, hidden_dim, bias=False),\n",
    "                LIF(tau_mem=tau_mem, activation_fn=act_fn),\n",
    "                nn.Linear(hidden_dim, 1, bias=False),\n",
    "                LIF(tau_mem=tau_mem, activation_fn=act_fn),\n",
    "            ).cuda()\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "input_spikes = (torch.rand(1, n_time_steps, encoding_dim) > 0.95).float().cuda()\n",
    "target = torch.zeros((1, n_time_steps, 1)).float().cuda()\n",
    "target[0, torch.randint(n_time_steps//5, n_time_steps, (4, )), 0] = 1\n",
    "\n",
    "out_spikes = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optimiser.zero_grad()\n",
    "    model[1].zero_grad()\n",
    "    model[3].zero_grad()\n",
    "    out = model(input_spikes)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    out_spikes.append(out.flatten())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "print(out.sum())\n",
    "input_spikes = input_spikes.detach().cpu().int().squeeze(0).numpy()\n",
    "output_spikes = torch.stack(out_spikes).detach().cpu().numpy().T\n",
    "target_spikes = target.detach().cpu().int().squeeze(0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "for spike in np.where(target_spikes)[0]:\n",
    "    ax1.axhspan(spike, spike, alpha=0.8, linewidth=2, color='red')\n",
    "ax1.scatter(np.where(output_spikes)[1], np.where(output_spikes)[0], s=1.)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6cb2531ff3209080f8ff5f4f1b83a3f6fd522559ade981afeb04664418b3902"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('slayer-comparison': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
