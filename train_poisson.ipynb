{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from poisson_models import ExodusNet, SlayerNet, smooth\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 250\n",
    "hidden_dim = 25\n",
    "spike_threshold = 1\n",
    "learning_rate = 1e-3\n",
    "n_time_steps = 200\n",
    "epochs = 3000\n",
    "width_grad = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_spikes, target):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    out_spikes = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimiser.zero_grad()\n",
    "        if model.spiking_layers:\n",
    "            for layer in model.spiking_layers:\n",
    "                layer.zero_grad()\n",
    "        out = model(input_spikes)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        out_spikes.append(out.flatten())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    output_spikes = torch.stack(out_spikes).detach().cpu().numpy().T\n",
    "    return output_spikes, np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8935)\n",
    "\n",
    "n_experiments = 8\n",
    "taus = np.logspace(2, 7, num=3, endpoint=True, base=2)\n",
    "scales = np.logspace(-2, 2, num=5, endpoint=True, base=10)\n",
    "print(taus)\n",
    "print(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exodus_param_losses = []\n",
    "slayer_param_losses = []\n",
    "for tau_mem in tqdm(taus):\n",
    "    for scale_grad in scales:\n",
    "        exodus_all_losses = np.zeros((epochs))\n",
    "        slayer_all_losses = np.zeros((epochs))\n",
    "        for i in range(n_experiments):\n",
    "            slayernet = SlayerNet(\n",
    "                encoding_dim=encoding_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                tau_mem=tau_mem,\n",
    "                spike_threshold=spike_threshold,\n",
    "                n_time_steps=n_time_steps,\n",
    "                width_grad=width_grad,\n",
    "                scale_grad=scale_grad,\n",
    "            ).cuda()\n",
    "\n",
    "            exodusnet = ExodusNet(\n",
    "                encoding_dim=encoding_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                tau_mem=tau_mem,\n",
    "                spike_threshold=spike_threshold,\n",
    "                n_time_steps=n_time_steps,\n",
    "                width_grad=width_grad,\n",
    "                scale_grad=scale_grad,\n",
    "            ).cuda()\n",
    "            \n",
    "            exodusnet.lin1.weight.data = slayernet.lin1.weight.data.clone()\n",
    "            exodusnet.lin2.weight.data = slayernet.lin2.weight.data.clone()\n",
    "\n",
    "            input_spikes = (torch.rand(1, n_time_steps, encoding_dim, 1, 1) > 0.95).float().cuda()\n",
    "            target = torch.zeros((1, n_time_steps, 1, 1, 1)).float().cuda()\n",
    "            target[0, torch.randint(n_time_steps//5, n_time_steps, (4, )), 0] = 1\n",
    "            \n",
    "            out1 = exodusnet(input_spikes)\n",
    "            out2 = slayernet(input_spikes)\n",
    "\n",
    "            assert torch.allclose(exodusnet.lif1.v_mem_recorded, slayernet.psp_post1, atol=1e-3)\n",
    "\n",
    "            slayer_output, slayer_losses = train(slayernet, input_spikes, target)\n",
    "            exodus_output, exodus_losses = train(exodusnet, input_spikes, target)\n",
    "\n",
    "            exodus_all_losses += exodus_losses\n",
    "            slayer_all_losses += slayer_losses\n",
    "        exodus_param_losses.append([exodus_all_losses, {'tau_mem': tau_mem, 'scale_grad': scale_grad}])\n",
    "        slayer_param_losses.append([slayer_all_losses, {'tau_mem': tau_mem, 'scale_grad': scale_grad}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "xpad = epochs/50\n",
    "xmin = -xpad\n",
    "xmax = epochs+xpad\n",
    "xmin_fraction = 1-(epochs/(epochs+xpad))\n",
    "xmax_fraction = 1-xmin_fraction\n",
    "\n",
    "legend1 = [\n",
    "    Line2D([0], [0], color='C0', lw=4, label='EXODUS spikes'),\n",
    "    Line2D([0], [0], color='C1', lw=4, label='SLAYER spikes'),\n",
    "    Line2D([0], [0], color='black', lw=4, label='Target'),\n",
    "]\n",
    "\n",
    "for spike in np.where(target.to(\"cpu\"))[1]:\n",
    "    ax1.axhspan(spike, spike, xmin=xmin_fraction, xmax=xmax_fraction, alpha=1, linewidth=3, color='black')\n",
    "ax1.scatter(np.where(exodus_output)[1], np.where(exodus_output)[0], s=0.3, alpha=0.5)\n",
    "ax1.scatter(np.where(slayer_output)[1], np.where(slayer_output)[0], s=0.3, alpha=0.5)\n",
    "ax1.set_ylabel(\"Spike output [t]\")\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.legend(handles=legend1, loc='best')\n",
    "\n",
    "legend2 = [\n",
    "    Line2D([0], [0], color='C0', lw=4, label='EXODUS smoothed loss'),\n",
    "    Line2D([0], [0], color='C1', lw=4, label='SLAYER smoothed loss'),\n",
    "]\n",
    "ax2 = fig.add_subplot(212)\n",
    "# ax2.plot(smooth(exodus_losses, window_len=30))\n",
    "# ax2.plot(smooth(slayer_losses, window_len=30))\n",
    "ax2.plot(smooth(exodus_param_losses[-1][0], window_len=30))\n",
    "ax2.plot(smooth(slayer_param_losses[-1][0], window_len=30))\n",
    "ax2.legend(handles=legend2)\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "\n",
    "plt.savefig(\"poisson_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "\n",
    "for loss, params in exodus_param_losses:\n",
    "    ax1.plot(smooth(loss, window_len=30), label=f\"tau_mem: {params['tau_mem']}, scale_grad: {params['scale_grad']}\")\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_ylim(top=0.3)\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "for loss, params in slayer_param_losses:\n",
    "    ax2.plot(smooth(loss, window_len=30), label=f\"tau_mem: {params['tau_mem']}, scale_grad: {params['scale_grad']}\")\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_ylim(top=0.3)\n",
    "\n",
    "plt.savefig(\"poisson_param_sweep.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "for (loss1, params), (loss2, params) in zip(exodus_param_losses, slayer_param_losses):\n",
    "    ax1.plot(smooth(loss1-loss2, window_len=30), label=f\"tau_mem: {params['tau_mem']}, scale_grad: {params['scale_grad']}\")\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "plt.savefig(\"poisson_param_sweep_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel('scale_grad')\n",
    "\n",
    "exodus_loss_integrals = np.array([loss.sum() for (loss, param) in exodus_param_losses])\n",
    "slayer_loss_integrals = np.array([loss.sum() for (loss, param) in slayer_param_losses])\n",
    "\n",
    "vmax = np.maximum(exodus_loss_integrals.max(), slayer_loss_integrals.max())\n",
    "\n",
    "for (loss1, params), (loss2, params) in zip(exodus_param_losses, slayer_param_losses):\n",
    "    ax1.scatter(params['tau_mem'], params['scale_grad'], c=loss1.sum(), s=100., vmin=0, vmax=vmax)\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('tau_mem')\n",
    "ax2.set_ylabel('scale_grad')\n",
    "for (loss1, params), (loss2, params) in zip(exodus_param_losses, slayer_param_losses):\n",
    "    ax = ax2.scatter(params['tau_mem'], params['scale_grad'], c=loss2.sum(), s=100., vmin=0, vmax=vmax)\n",
    "\n",
    "print(loss2)\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(ax, cax=cbar_ax)\n",
    "\n",
    "plt.savefig(\"poisson_param_sweep_loss_integral.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6cb2531ff3209080f8ff5f4f1b83a3f6fd522559ade981afeb04664418b3902"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('slayer-comparison': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
