{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7272506",
   "metadata": {},
   "source": [
    "*Note: This is an adaptation of the notebook found in optical_flow repo, commit `b654828`, which in its original form works with sinabs from commit `3fafee4` and sinabs-slayer, commit `7114a11`*\n",
    "\n",
    "This adaptation should work with sinabs and exodus in their most recent versions (25.02.2022) and is independent of the optical_flow project. \n",
    "\n",
    "# Why does the slayer model not work?\n",
    "\n",
    "This notebook will show that for the wheel-motion classification toy task, the same model architecture can be trained with exodus but training on slayer fails. We will try to explore why this is the case.\n",
    "\n",
    "The notebook's code is based on the script `binary_task.py` from the optical_flow repo.\n",
    "\n",
    "SPOILER:\n",
    "The problem was exploding gradients in the slayer model. Enabling an option to scale down the surrogate gradients resolved it.\n",
    "The problem can be reproduced by setting `kwargs_spiking[\"scale_grads\"]` to 1 (second code cell), which corresponds to the original value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ca1fa-85ac-41ff-a4ad-e39754b414da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Run this cell only to generate the data!\n",
    "%run data_generation.py --size=256 --num_segments=4 --num_timesteps=300 --save_path=rotating_wedge_events.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Imports\n",
    "\n",
    "# Set this to inline or notebook if widget is not supported. Animation might not work then.\n",
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from slayerSNN import loss as SpikeLoss\n",
    "\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.utils import get_activations\n",
    "import sinabs.layers as sl\n",
    "\n",
    "from data import InvertDirDataset\n",
    "from binary_models import SlayerModel, ExodusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed2ced-8bfd-43aa-9517-5710f2ddd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Data loading and inspection\n",
    "raster = np.load(\"rotating_wedge_events.npy\").astype(np.float32)\n",
    "raster = raster.transpose(1,2,3,0)\n",
    "print(raster.shape)\n",
    "\n",
    "raster_merged = (raster[1] - raster[0]).transpose(-1,0,1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "screen = ax.imshow(raster_merged[0], vmin=np.min(raster_merged), vmax=np.max(raster_merged))\n",
    "\n",
    "def update_plot(frm):\n",
    "    return screen.set_data(frm)\n",
    "\n",
    "# anim = FuncAnimation(fig, update_plot, frames=raster_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89183ec-f3f2-4597-8f77-11f63a6e0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Settings and hyperparameters\n",
    "lr = 1e-3\n",
    "num_epochs = 40\n",
    "optimizer_class = torch.optim.SGD  # torch.optim.Adam\n",
    "downsample = 1\n",
    "num_ts = raster.shape[-1] // downsample\n",
    "\n",
    "# - Model parameters\n",
    "kwargs_model = {\n",
    "    \"grad_width\": 0.5,  # 0.5\n",
    "    \"grad_scale\": 1,  # 0.02\n",
    "    \"thr\": 1,\n",
    "    \"num_ts\": num_ts,\n",
    "}\n",
    "\n",
    "# Use LIF model\n",
    "# kwargs_model[\"neuron_type\"] = \"LIF\"\n",
    "# kwargs_model[\"tau_leak\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Datset and loader\n",
    "# Set sample_size and step_size such that each sample corresponds to all frames of one class\n",
    "ds = InvertDirDataset(\n",
    "    raster, sample_size=num_ts, step_size=num_ts, downsample=downsample\n",
    ")\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "# - Loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# - Sinabs model\n",
    "model_exodus = ExodusModel(**kwargs_model).cuda()\n",
    "model_exodus.reset()\n",
    "print(\"Sinabs model:\")\n",
    "print(model_exodus)\n",
    "\n",
    "model_slayer = SlayerModel(**kwargs_model).cuda()\n",
    "print(\"Slayer model:\")\n",
    "print(model_slayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95482f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Transfer weights from exodus to slayer model to ensure same initial conditions\n",
    "model_slayer.conv0.weight.data = model_exodus.conv0.weight.data.unsqueeze(-1).clone()\n",
    "model_slayer.conv1.weight.data = model_exodus.conv1.weight.data.unsqueeze(-1).clone()\n",
    "model_slayer.linear.weight.data = model_exodus.linear.weight.data.clone().reshape(2, 8, 4, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e51de",
   "metadata": {},
   "source": [
    "Now the two models should produce the same output for a given input. Let's make sure this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8c361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Compare outputs before training\n",
    "outputs_slayer = []\n",
    "outputs_exodus = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Load input from dataset to avoid shuffling.\n",
    "    for inp, *__ in ds:\n",
    "        # Add batch dimension\n",
    "        inp = inp.unsqueeze(0).cuda()\n",
    "        outputs_slayer.append(model_slayer(inp))\n",
    "        outputs_exodus.append(model_exodus(inp))\n",
    "        model_exodus.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8a2c7-af18-46f7-ac09-a671fa600402",
   "metadata": {},
   "outputs": [],
   "source": [
    "os = outputs_slayer[1].squeeze(0).cpu().detach()\n",
    "oe = outputs_exodus[1].squeeze(0).cpu().detach()\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.plot(oe[0].t(), lw=2, label=\"exodus\")\n",
    "ax1.plot(os[0].t(), ls=\"--\", label=\"slayer\")\n",
    "ax2.plot(oe[1].t(), lw=2, label=\"exodus\")\n",
    "ax2.plot(os[1].t(), ls=\"--\", label=\"slayer\")\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee2491",
   "metadata": {},
   "source": [
    "Outputs seem very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8a7b4-463f-4a64-a998-f195609b7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(a, b, eps=1e-16):\n",
    "    a = a.flatten()\n",
    "    b= b.flatten()\n",
    "    return torch.sum(a*b) / (torch.sqrt(torch.sum(a**2) * torch.sum(b**2)) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1877f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Set up training\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim_exo = optimizer_class(model_exodus.parameters(), lr=lr)\n",
    "optim_slr = optimizer_class(model_slayer.parameters(), lr=lr)\n",
    "\n",
    "# - Do a single batch\n",
    "dl_iter = iter(dl)\n",
    "inp, tgt, onehot = next(dl_iter)\n",
    "print(\"Target:\", tgt)\n",
    "inp = inp.cuda()\n",
    "\n",
    "# Average over different initialisations\n",
    "all_max_grads_exo = []\n",
    "all_max_grads_slr = []\n",
    "all_std_grads_exo = []\n",
    "all_std_grads_slr = []\n",
    "all_corrs = []\n",
    "for seed in [11, 22, 33, 44, 55, 66, 77, 88, 99]:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model_exodus = ExodusModel(**kwargs_model).cuda()\n",
    "    model_exodus.reset()\n",
    "\n",
    "    model_slayer = SlayerModel(**kwargs_model).cuda()\n",
    "\n",
    "    # - Transfer weights from exodus to slayer model to ensure same initial conditions\n",
    "    model_slayer.conv0.weight.data = model_exodus.conv0.weight.data.unsqueeze(-1).clone()\n",
    "    model_slayer.conv1.weight.data = model_exodus.conv1.weight.data.unsqueeze(-1).clone()\n",
    "    model_slayer.linear.weight.data = model_exodus.linear.weight.data.clone().reshape(2, 8, 4, 4, 1)\n",
    "\n",
    "    # Sinabs\n",
    "    out_exo = model_exodus(inp).sum(-1).cpu()\n",
    "    optim_exo.zero_grad()\n",
    "    loss_exo = loss_func(out_exo, tgt)\n",
    "    loss_exo.backward()\n",
    "    grads_exo = [p.grad for p in model_exodus.parameters()]\n",
    "    model_exodus.reset()\n",
    "\n",
    "    # Slayer\n",
    "    out_slr = model_slayer(inp).sum(-1).cpu()\n",
    "    optim_slr.zero_grad()\n",
    "    loss_slr = loss_func(out_slr, tgt)\n",
    "    loss_slr.backward()\n",
    "    grads_slr = [p.grad.squeeze(-1) for p in model_slayer.parameters() if p.grad is not None]\n",
    "\n",
    "    print(f\"Losses - Sinabs: {loss_exo.item()}, Slayer: {loss_slr.item()}\")\n",
    "\n",
    "    max_grads_exo = [torch.max(g).item() for g in grads_exo]\n",
    "    max_grads_slr = [torch.max(g).item() for g in grads_slr]\n",
    "    std_grads_exo = [torch.std(g).item() for g in grads_exo]\n",
    "    std_grads_slr = [torch.std(g).item() for g in grads_slr]\n",
    "    correlations = [correlation(gs, ge).item() for gs, ge in zip(grads_slr, grads_exo)]\n",
    "\n",
    "    all_max_grads_exo.append(max_grads_exo)\n",
    "    all_max_grads_slr.append(max_grads_slr)\n",
    "    all_std_grads_exo.append(std_grads_exo)\n",
    "    all_std_grads_slr.append(std_grads_slr)\n",
    "    all_corrs.append(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_max_grads_exo = torch.tensor(all_max_grads_exo).mean(dim=0)\n",
    "avg_max_grads_slr = torch.tensor(all_max_grads_slr).mean(dim=0)\n",
    "avg_std_grads_exo = torch.tensor(all_std_grads_exo).mean(dim=0)\n",
    "avg_std_grads_slr = torch.tensor(all_std_grads_slr).mean(dim=0)\n",
    "avg_corrs = torch.tensor(all_corrs).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "labels = ['layer1', 'layer2', 'layer3']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax1.bar(x - width/2, avg_max_grads_slr, width, label='SLAYER')\n",
    "ax1.bar(x + width/2, avg_max_grads_exo, width, label='EXODUS')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('max gradient')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.bar(x - width/2, avg_std_grads_slr, width, label='SLAYER')\n",
    "ax2.bar(x + width/2, avg_std_grads_exo, width, label='EXODUS')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('gradient std')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.bar(range(len(correlations)), avg_corrs, color='C3')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(labels)\n",
    "ax3.set_title('gradient correlation\\nbetween slayer and exodus')\n",
    "ax3.set_ylim(bottom=0.5)\n",
    "\n",
    "fig.suptitle('NEURON MODEL IAF')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gradient_comparison_optical_flow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dfcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b308aa1a",
   "metadata": {},
   "source": [
    "It turns out that with the default scaling (`kwargs_spiking[\"scale_grads\"] = 1.0`), the gradients in the slayer model explode. After trying a few values, setting the scale to 0.02 seems to give reasonable gradients.\n",
    "Let's train both models to see, whether everything works now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49526396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Training loop\n",
    "\n",
    "mistakes_exo = []\n",
    "mistakes_slr = []\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    print(f\"Epoch {ep} ------------------------------------------------------\")\n",
    "    for inp, tgt, __ in dl:\n",
    "        inp = inp.cuda()       \n",
    "        \n",
    "        # Sinabs\n",
    "        out_exo = model_exodus(inp).sum(-1).cpu()\n",
    "        __, predict_exo = torch.max(out_exo, 1)\n",
    "        optim_exo.zero_grad()\n",
    "        loss_exo = loss_func(out_exo, tgt)\n",
    "        loss_exo.backward()\n",
    "        exo_right = tgt.item() == predict_exo.item()\n",
    "        mistakes_exo.append(int(not exo_right))\n",
    "\n",
    "        # Slayer\n",
    "        out_slr = model_slayer(inp).sum(-1).cpu()\n",
    "        __, predict_slr = torch.max(out_slr, 1)\n",
    "        optim_slr.zero_grad()\n",
    "        loss_slr = loss_func(out_slr, tgt)\n",
    "        loss_slr.backward()        \n",
    "        slr_right = tgt.item() == predict_slr.item()\n",
    "        mistakes_slr.append(int(not slr_right))\n",
    "        \n",
    "        # Get correlation\n",
    "        grads_exo = [p.grad.squeeze(-1) for p in model_exodus.parameters() if p.grad is not None]\n",
    "        grads_slr = [p.grad.squeeze(-1) for p in model_slayer.parameters() if p.grad is not None]\n",
    "        correls = [correlation(gs, ge).item() for gs, ge in zip(grads_slr, grads_exo)]\n",
    "        \n",
    "        # Optimizer step\n",
    "        optim_exo.step()\n",
    "        model_exodus.reset()\n",
    "        optim_slr.step()\n",
    "        \n",
    "        # Print statement\n",
    "        print(f\"Target: {tgt.item()}\")\n",
    "        print(f\"Prediction exodus: {predict_exo.item()} ({'correct' if exo_right else 'wrong'})\")\n",
    "        print(f\"Prediction slayer: {predict_slr.item()} ({'correct' if slr_right else 'wrong'})\")\n",
    "        print(f\"Gradient correlation: {correls}\")\n",
    "                \n",
    "    # print(ep, sum(tl[-5:]), end=\"\\r\")\n",
    "    print(f\"Total mistakes exodus: {sum(mistakes_exo)}, slayer: {sum(mistakes_slr)} -----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(mistakes_exo)\n",
    "plt.plot(mistakes_slr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea82b4-5842-4a91-8845-3d0e37d1245e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
